{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Importing the LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import langchain\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "# from langchain import PineconeVectorStore\n",
    "# from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()##loads all the environment variables from the .env file\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the document from the documents folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the document\n",
    "def read_doc(directory):\n",
    "    file_loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = file_loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = read_doc(\"documents/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the docs into chunks as there is token limitation\n",
    "\n",
    "def chunk_data(docs, chunk_size = 800, chunk_overlap = 50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap)\n",
    "    docs = text_splitter.split_documents(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = chunk_data(docs=doc)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# openai_key = \"\"\n",
    "# pinecone_api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMBEDDING the data from the document for uploading on the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001D2F9A56050>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001D2F9A60990>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='sk-proj-JWNqVzLG40YZcZilB8OlT3BlbkFJf8OBWkPX1StzhqEjkt0l', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding technique of OPENAI\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = embeddings.embed_query(\"How are you?\")\n",
    "len(vectors)#just an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the embeddings to Pinecone database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "index_name = \"langchainvector\"\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = PineconeVectorStore.from_documents(doc, index_name=index_name, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the query to the db and get some similar content from the document in the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Consine Similarity Retrieve Results\n",
    "##this is the function to get the data from the vector db\n",
    "\n",
    "def retrieve_query(query, k=2):\n",
    "    matching_results = index.similarity_search(query, k=k)\n",
    "    return matching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the LLM and chain(connecting the steps of query retrieval and the api call to the LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\Vectordb\\env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.5, max_tokens=100)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain to connect the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search answers from vector db\n",
    "def retrieve_answers(query):\n",
    "    doc_search = retrieve_query(query)\n",
    "    print(doc_search)\n",
    "    response = chain.run(input_documents=doc_search, question=query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"Dinesh\\nBalivada\\nBangaluru,\\nIndia\\n•\\n+91\\n7993438995\\n•\\ndineshbalivada8@gmail.com\\nLinkedIn\\nProfile\\n•\\nGitHub\\nProfile\\n—————————————————————————————————————————————————\\nE\\nDUCATION\\nIndian\\nInstitute\\nof\\nTechnology\\nMadras\\nChennai,\\nIndia\\nBachelor\\nof\\nScience,\\nData\\nScience\\nand\\nProgramming\\n2021-2025\\nThe\\nNational\\nInstitute\\nof\\nEngineering\\nMysuru,\\nIndia\\nBachelor\\nof\\nEngineering,\\nComputer\\nScience\\nand\\nEngineering\\n2020-2024\\n—————————————————————————————————————————————————\\nW\\nORK\\nE\\nXPERIENCE\\nN\\nOKIA\\nN\\nETWORKS\\nA\\nND\\nS\\nOLUTIONS\\n2024\\n|\\nBangaluru,\\nIndia\\n•\\nCurrently\\nworking\\nas\\nAugmented\\nAutomation\\nStudent\\nTrainee\\nfor\\na\\nperiod\\nof\\n6\\nmonths.\\n•\\nHands-on\\nexperience\\non\\nMicrosoft\\nPower\\nPlatforms\\n(including\\nPower\\nBI)\\nand\\ntrainings\\non\\nMicrosoft\\nAzure\\nCloud\\nServices.\\n•\\nOrchestrated\\na\\ntransformative\\nproject\\nleveraging\\nPower\\nApps\\nand\\nPower\\nAutomate\\nsoftware\\nto\\nincorporate\\nLarge\\nLanguage\\nModels\\nfor\\nsecurity\\ngovernance\\nand\\napplication\\nevaluation,\\nstreamlining\\nprocesses\\nand\\nsaving\\n25\\nhours\\nweekly\\nin\\nreview\\ntime.\\n•\\nFacilitated\\nin\\nframing\\nthe\\ndesign\\ndocuments\\n,\\ndevelopment\\nand\\nanalytics\\nof\\nautomated\\nframework\\naround\\nLLM\\nand\\nit's\\noutput\\nwhich\\nachieved\\n88%\\naccuracy.\\n•\\nDeveloped\\nand\\nintegrated\\nthe\\nframework\\nin\\ncollaboration\\nwith\\nAutomation\\nand\\nSoftware\\nDevelopment\\nteams\\nto\\ntest\\napplications\\nthrough\\nLLM.\\nP\\nROJECTS\\nA\\nND\\nA\\nCHIEVEMENTS\\nC\\nUSTOMER\\nR\\nEVIEW\\nMLOPS\\n2024\\n|\\nBangaluru,\\nIndia\\n●\\nImplemented\\nLinear\\nRegression\\non\\na\\nsmall\\ndataset\\nof\\nCustomer\\nReviews\\npredicting\\nthe\\nreview\\nscore\\nfor\\nthe\\nnext\\norder\\nor\\npurchase.\\n●\\nThe\\nmotive\\nof\\nthe\\nproject\\nis\\nto\\nimplement\\nthe\\nconcepts\\nof\\nMLOps\\nusing\\nZenML\\nand\\nMLFlow.\\n●\\nThe\\nproject\\nhas\\ntwo\\npipelines\\nnamely\\nTraining\\nand\\nDeployment\\nPipeline.\\n●\\nThe\\nTraining\\npipeline\\nimplements\\ningest\\ndata,\\nclean\\ndata,\\ntrain\\nmodel,\\nevaluation\\nand\\nis\\nvisualized\\nin\\nZenML\\ndashboard.\\n●\\nThe\\nDeployment\\nwas\\ndone\\nlocally\\nwith\\nthe\\nhelp\\nof\\nMLFlow.\\n●\\nCustomer\\nReview\", metadata={'page': 0.0, 'source': 'documents\\\\Dinesh_Balivada_Resume.docx.pdf'}), Document(page_content=\"Dinesh\\nBalivada\\nBangaluru,\\nIndia\\n•\\n+91\\n7993438995\\n•\\ndineshbalivada8@gmail.com\\nLinkedIn\\nProfile\\n•\\nGitHub\\nProfile\\n—————————————————————————————————————————————————\\nE\\nDUCATION\\nIndian\\nInstitute\\nof\\nTechnology\\nMadras\\nChennai,\\nIndia\\nBachelor\\nof\\nScience,\\nData\\nScience\\nand\\nProgramming\\n2021-2025\\nThe\\nNational\\nInstitute\\nof\\nEngineering\\nMysuru,\\nIndia\\nBachelor\\nof\\nEngineering,\\nComputer\\nScience\\nand\\nEngineering\\n2020-2024\\n—————————————————————————————————————————————————\\nW\\nORK\\nE\\nXPERIENCE\\nN\\nOKIA\\nN\\nETWORKS\\nA\\nND\\nS\\nOLUTIONS\\n2024\\n|\\nBangaluru,\\nIndia\\n•\\nCurrently\\nworking\\nas\\nAugmented\\nAutomation\\nStudent\\nTrainee\\nfor\\na\\nperiod\\nof\\n6\\nmonths.\\n•\\nHands-on\\nexperience\\non\\nMicrosoft\\nPower\\nPlatforms\\n(including\\nPower\\nBI)\\nand\\ntrainings\\non\\nMicrosoft\\nAzure\\nCloud\\nServices.\\n•\\nOrchestrated\\na\\ntransformative\\nproject\\nleveraging\\nPower\\nApps\\nand\\nPower\\nAutomate\\nsoftware\\nto\\nincorporate\\nLarge\\nLanguage\\nModels\\nfor\\nsecurity\\ngovernance\\nand\\napplication\\nevaluation,\\nstreamlining\\nprocesses\\nand\\nsaving\\n25\\nhours\\nweekly\\nin\\nreview\\ntime.\\n•\\nFacilitated\\nin\\nframing\\nthe\\ndesign\\ndocuments\\n,\\ndevelopment\\nand\\nanalytics\\nof\\nautomated\\nframework\\naround\\nLLM\\nand\\nit's\\noutput\\nwhich\\nachieved\\n88%\\naccuracy.\\n•\\nDeveloped\\nand\\nintegrated\\nthe\\nframework\\nin\\ncollaboration\\nwith\\nAutomation\\nand\\nSoftware\\nDevelopment\\nteams\\nto\\ntest\\napplications\\nthrough\\nLLM.\\nP\\nROJECTS\\nA\\nND\\nA\\nCHIEVEMENTS\\nC\\nUSTOMER\\nR\\nEVIEW\\nMLOPS\\n2024\\n|\\nBangaluru,\\nIndia\\n●\\nImplemented\\nLinear\\nRegression\\non\\na\\nsmall\\ndataset\\nof\\nCustomer\\nReviews\\npredicting\\nthe\\nreview\\nscore\\nfor\\nthe\\nnext\\norder\\nor\\npurchase.\\n●\\nThe\\nmotive\\nof\\nthe\\nproject\\nis\\nto\\nimplement\\nthe\\nconcepts\\nof\\nMLOps\\nusing\\nZenML\\nand\\nMLFlow.\\n●\\nThe\\nproject\\nhas\\ntwo\\npipelines\\nnamely\\nTraining\\nand\\nDeployment\\nPipeline.\\n●\\nThe\\nTraining\\npipeline\\nimplements\\ningest\\ndata,\\nclean\\ndata,\\ntrain\\nmodel,\\nevaluation\\nand\\nis\\nvisualized\\nin\\nZenML\\ndashboard.\\n●\\nThe\\nDeployment\\nwas\\ndone\\nlocally\\nwith\\nthe\\nhelp\\nof\\nMLFlow.\\n●\\nCustomer\\nReview\", metadata={'page': 0.0, 'source': 'documents\\\\Dinesh_Balivada_Resume.docx.pdf'})]\n",
      "\n",
      "\n",
      "NOKIA NETWORKS AND SOLUTIONS\n",
      "Augmented Automation Student Trainee | Bangaluru, India | 2024\n",
      "• Currently employed for a 6-month period as an Augmented Automation Student Trainee.\n",
      "• Hands-on experience with Microsoft Power Platforms (including Power BI) and training on Microsoft Azure Cloud Services.\n",
      "• Orchestrated a transformative project utilizing Power Apps and Power Automate software to incorporate Large Language Models for security governance and application evaluation, resulting in streamlined processes and a weekly\n"
     ]
    }
   ],
   "source": [
    "our_query = \"Read the work experience at Nokia and rewrite it professionally\"\n",
    "answer = retrieve_answers(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
